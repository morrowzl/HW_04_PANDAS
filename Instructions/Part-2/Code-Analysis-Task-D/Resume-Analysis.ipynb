{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Analysis\n",
    "_**HARD: This is a curveball assignment. Plus, this is Python without Pandas.**_\n",
    "\n",
    "#### The objective of this assignment is for you to explain what is happening in each cell in clear, understandable language. \n",
    "\n",
    "#### _There is no need to code._ The code is there for you, and it already runs. Your task is only to explain what each line in each cell does.\n",
    "\n",
    "#### The placeholder cells should describe what happens in the cell below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports `os` as a dependency because the `os.path.join` function *must be used in this script*. Also, the `string` dependency is needed because later in the script, `string.punctuation` will be used to detect and remove punctuation symbols. The line \"from collections import Counter\" loads a specialized datatype, Counter, into the environment from the \"collections\" module. Counter is \"a dict subclass for counting hashable objects\" (python.org documentation). That is, counter will store an object's elements as keys in a dictionary, and it will store the count of the element's occurence in the object as the corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell uses the os library path.join function to create a filepath based on the user inputs, which is stored as variable \"resume_path\" (points to markdown file). The cell then defines two sets of required and desired skills to which resume contents will later be compared using set operations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################Also, why are the variables in ALL CAPS?]###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "resume_path = os.path.join(\".\", 'resume.md')\n",
    "\n",
    "# Skills to match\n",
    "REQUIRED_SKILLS = {\"excel\", \"python\", \"mysql\", \"statistics\"}\n",
    "DESIRED_SKILLS = {\"r\", \"git\", \"html\", \"css\", \"leaflet\", \"modeling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[Replace this with your clear explanation of what happens in the cell below. How is this function defined? What does it take in, how does it work, and what does it return?]_\n",
    "Cell calls defines the load_file function to read and relay information from the file at \"filepath\" (to be specified later). Input \"r\" indicates the file will be opened as read-only; it will initially be referred to as resume_file_handler. Once read, the contents of resume_file_handler will be stored as variable \"resume_contents\". The contents will be changed to lower case and stored as resume_contents a second time. The contents will then be split by the default delimeter \" \" (space) and stored as variable \"resume_tokens\". The function will conclude by retaining a list \"resume_tokens\" for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    # Helper function to read a file and return the data.\n",
    "    with open(filepath, \"r\") as resume_file_handler:\n",
    "        resume_contents = resume_file_handler.read()\n",
    "        resume_contents = resume_contents.lower()\n",
    "        resume_tokens = resume_contents.split()\n",
    "        return resume_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell passes the previously defined variable resume_path to the load_file function. Once the function has executed and the resume_tokens list is retained, its contents are stored as variable \"word_list\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text for a Resume\n",
    "word_list = load_file(resume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell creates an empty set called \"resume\" to contain unique entries based on the contents of the file processed by load_file (stored in word_list). The empty set will be populated via the for-loop, which adds a token to \"resume\" by looping as many times as there are things (tokens) in word_list. While the list \"word_list\" may contain duplicates, the set \"resume\" will not update for duplicate values encountered during the for-loop. After completing the for-loop, Python will print a string on a new line, which will head the successively printed contents of set \"resume\". A new set \"punctuation\" is defined as the contents of \"punctuation\" casted as strings. The contents of resume that are also found in punctuation (standalone punctuation characters) will be removed from the set by line 14. A string will be printed on a new line, which will head the successively printed (remaining) contents of resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS BEFORE MOVING PUNCTUATION\n",
      "{'and', 'files', 'social', 'mongodb', 'boot', 'performing', 'camp', 'tableau,', 'web', 'leaflet.js', 'forecasting', 'experience', 'apis.', 'visualizations', 'statistics,', 'excel.', 'machine', 'analytics', 'hadoop,', 'bootstrap,', 'microsoft', 'css,', 'git/github', 'basic', 'data', 'html,', 'tableau', '*', 'aws', 'to', 'vba', 'mining', 'cloud', 'apps', 'scripts', 'open-source', 'media', 'working', 'visualization', 'software', 'python,', 'developing', 'api', 'in', 'excel,', 'the', 'education', 'with', 'learning,', 'tables', 'graduate', 'from', 'javascript,', 'stein', 'advanced', 'front-end', 'creating', 'business', 'hadoop', 'databases', 'sets', 'algorithms', 'frank', 'r,', 'writing', 'pivot', 'intelligence', 'statistics', 'html/css,', 'using', 'n.', 'mysql', 'contributing', 'sql,', 'analyze', 'interactions,', 'd3,', 'interests', '##', 'pandas', '#', 'skills', 'modeling', 'big', 'd3', 'learning', 'python', 'mining,', 'designing'}\n",
      "\n",
      "WORDS AFTER MOVING PUNCTUATION\n",
      "{'files', 'and', 'social', 'mongodb', 'boot', 'performing', 'camp', 'tableau,', 'web', 'leaflet.js', 'forecasting', 'experience', 'apis.', 'visualizations', 'statistics,', 'excel.', 'machine', 'analytics', 'hadoop,', 'bootstrap,', 'microsoft', 'css,', 'git/github', 'basic', 'data', 'html,', 'tableau', 'aws', 'to', 'vba', 'mining', 'cloud', 'apps', 'scripts', 'open-source', 'media', 'working', 'visualization', 'software', 'python,', 'developing', 'api', 'in', 'excel,', 'the', 'education', 'with', 'learning,', 'tables', 'graduate', 'from', 'javascript,', 'stein', 'advanced', 'front-end', 'creating', 'business', 'hadoop', 'databases', 'sets', 'algorithms', 'frank', 'r,', 'writing', 'pivot', 'intelligence', 'statistics', 'html/css,', 'using', 'n.', 'mysql', 'contributing', 'sql,', 'analyze', 'interactions,', 'd3,', 'interests', '##', 'pandas', 'skills', 'modeling', 'big', 'd3', 'learning', 'python', 'mining,', 'designing'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words from the resume\n",
    "resume = set()\n",
    "\n",
    "# HINT: Single elements in a programming language are often referred to as tokens\n",
    "for token in word_list:\n",
    "    resume.add(token)\n",
    "\n",
    "print('\\nWORDS BEFORE MOVING PUNCTUATION')    \n",
    "print(resume)\n",
    "\n",
    "# Remove Punctuation that were read as whole words\n",
    "punctuation = set(string.punctuation)\n",
    "# HINT: Attributes that are in `resume` that are not in `punctuation` (difference)\n",
    "resume = resume - punctuation\n",
    "\n",
    "print('\\nWORDS AFTER MOVING PUNCTUATION')    \n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the cell will print the required skills header string, and then print the elements common among both \"resume\" and \"required skills\" sets, which results from the set intersection operation. Next, the cell will print the desired skills header string, and then print the elements common among both the \"resume\" and \"desired skills\" sets, which results from the set intersection operation. Next, word_list is refined by a list comprehension which for-loops through conditionals and an if-statement to remove standalone punctuation and retain each element only if it is not contained in the string.punctuation set. A header string is printed on a new line, followed by the once-refined word_list. Next, word_list is further refined by an additional list comprehension to remove punctuation marks within words. The list comprehension for-loops through each word in word_list. On each word, the procedure will for-loop through conditionals and an if-statement to remove punctuation characters. If the punctuation character is not contained in the string.punctuation set, the two word halves will be joined by nothing (combined). If the character is an element of the string.punctuation set, it will not be retained (i.e. stored again in word list). Next, a header string is printed on a new line, followed by the twice-refined word_list. After that, list comprehension is used a third time to remove stop words, which can be altered to remove or not-remove whichever words the user specifies. After the third refinement of word_list, another header string is printed to a new line, followed by the thrice-refined word_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUIRED SKILLS\n",
      "{'statistics', 'mysql', 'python'}\n",
      "DESIRED SKILLS\n",
      "{'modeling'}\n",
      "\n",
      "WORD LIST AFTER PUNCTUATION REMOVAL\n",
      "['frank', 'n.', 'stein', '##', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '##', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel.', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis.', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'front-end', 'web', 'visualizations', 'using', 'html,', 'css,', 'bootstrap,', 'd3,', 'and', 'leaflet.js', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '##', 'skills', 'microsoft', 'excel,', 'python,', 'javascript,', 'html/css,', 'api', 'interactions,', 'social', 'media', 'mining,', 'sql,', 'hadoop,', 'tableau,', 'advanced', 'statistics,', 'machine', 'learning,', 'r,', 'git/github', '##', 'interests', 'contributing', 'to', 'open-source', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html,', 'css,', 'javascript,', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER CHARACTER PUNCTUATION REMOVAL\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'using', 'html', 'css', 'bootstrap', 'd3', 'and', 'leafletjs', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'to', 'opensource', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html', 'css', 'javascript', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER STOP WORDS\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'vba', 'scripts', 'excel', 'modeling', 'forecasting', 'data', 'basic', 'statistics', 'writing', 'python', 'scripts', 'analyze', 'data', 'sets', 'from', 'files', 'apis', 'social', 'media', 'mining', 'python', 'mysql', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'html', 'css', 'bootstrap', 'd3', 'leafletjs', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'hadoop', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'opensource', 'software', 'data', 'analytics', 'python', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'html', 'css', 'javascript', 'd3', 'big', 'data', 'the', 'cloud', 'aws']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Required Skills Match using Set Intersection\n",
    "print('REQUIRED SKILLS')\n",
    "print(resume & REQUIRED_SKILLS)\n",
    "\n",
    "# Calculate the Desired Skills Match using Set Intersection\n",
    "print('DESIRED SKILLS')\n",
    "print(resume & DESIRED_SKILLS)\n",
    "\n",
    "\n",
    "# Word Punctuation Cleaning\n",
    "word_list = [word for word in word_list if word not in string.punctuation]\n",
    "print('\\nWORD LIST AFTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Character Punctuation Cleaning\n",
    "word_list = [''.join(char for char in word if char not in string.punctuation) for word in word_list]\n",
    "print('\\nWORD LIST AFTER CHARACTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Clean Stop Words\n",
    "stop_words = [\"and\", \"with\", \"using\", \"##\", \"working\", \"in\", \"to\"]\n",
    "word_list = [word for word in word_list if word not in stop_words]\n",
    "print('\\nWORD LIST AFTER STOP WORDS')\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with your clear explanation of what happens in the cell below.\n",
    "Be sure to explain the following:\n",
    "\n",
    "* How was the `word_count` dictionary initialized, what were in initial key values, and how were they set?\n",
    "* Explain the logic behind incrementing the world count value using the `for loop`. Be sure to explain how to reference the word key in the `word_count` dictionary\n",
    "* Collections.counter is optional, but explain the difference between the `for loop` and using `Counter`\n",
    "The word_count dictionary is initialized as an empty dictionary whose keys will be comprised of the elements in word_list. The for-loop will make a pass over each element of word_list and add one to the value corresponding to that key-word in the dictionary. Word_counter is defined as the output of applying the Counter datatype to the elements of word_list, which will also be a dictionary structure datatype. The Counter datatype makes a dictionary whose keys are the elements of the user-specified word_list and whose values are the count of the key's occurrences in the list. We expect (and double check) the for-loop and application of Counter to produce matching products, which tests to be true using the \"==\" equality test. The last two lines print a header string and a string to visually separate the header from the next output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Top 10 Words\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# Resume Word Count\n",
    "# ==========================\n",
    "# Initialize a dictionary with default values equal to zero\n",
    "word_count = {}.fromkeys(word_list, 0)\n",
    "\n",
    "# Loop through the word list and count each word.\n",
    "for word in word_list:\n",
    "    word_count[word] += 1\n",
    "# print(word_count)\n",
    "\n",
    "# Bonus using collections.Counter\n",
    "word_counter = Counter(word_list)\n",
    "# print(word_counter)\n",
    "\n",
    "# Comparing both word count solutions\n",
    "print(word_count == word_counter)\n",
    "\n",
    "# Top 10 Words\n",
    "print(\"Top 10 Words\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with your clear explanation of what happens in the cell below. Which column was sorted and how? How was the top ten selected? Does that explain the significance of `[:10]`?\n",
    "\n",
    "The cell initializes an empty list \"sorted_word\". The list is filled by the for-loop, which loops over each of the first 10 words in word_count (determined by the [:10]). For each iteration, the loop will look to the key in the word_count dictionary to retrieve the corresponding count value. The list will be reverse-ordered (i.e. descending). Next, an f-string will be printed that displays the token-word and its count value. The f-string columns are separated by twenty \"things\"(measurement).\n",
    "\n",
    "As a bonus, explain how you would get rid of the blank token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: data                                     Count: 7\n",
      "Token:                                          Count: 4\n",
      "Token: python                                   Count: 4\n",
      "Token: analytics                                Count: 3\n",
      "Token: visualization                            Count: 2\n",
      "Token: scripts                                  Count: 2\n",
      "Token: excel                                    Count: 2\n",
      "Token: statistics                               Count: 2\n",
      "Token: social                                   Count: 2\n",
      "Token: media                                    Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Sort words by count and print the top 10\n",
    "sorted_words = []\n",
    "for word in sorted(word_count, key=word_count.get, reverse=True)[:10]:\n",
    "    print(f\"Token: {wor20} Count: {word_count[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
